# sentence_processing.py

from flask import g, current_app
from flask_login import current_user
from rapidfuzz import fuzz
import re
from docx import Document
from spacy_manager import SpacyModel
from models import db, Paragraph, KeyWord, Report, HeadSentence, BodySentence, TailSentence, HeadSentenceGroup
from logger import logger
from collections import defaultdict



def extract_keywords_from_doc(file_path):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≥—Ä—É–ø–ø–∏—Ä—É—è –∏—Ö –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º –∏–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ.
    
    –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≥—Ä—É–ø–ø–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Å—Ç—Ä–æ–∫–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∑–∞–ø—è—Ç—ã–º–∏.
    –ñ–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–∏–≤—è–∑–∫—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∫ –ø—Ä–æ—Ç–æ–∫–æ–ª—É. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∂–∏—Ä–Ω—ã–º –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏–º–µ–Ω–µ–º 
    –ø—Ä–æ—Ç–æ–∫–æ–ª–∞, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å—á–∏—Ç–∞—é—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏.
    
    Args:
        file_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ Word.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç 'report_id' (ID –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∏–ª–∏ None –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤) 
              –∏ 'key_words' (—Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤).
    """
    doc = Document(file_path)
    keywords = []
    current_protocol = None  # –¢–µ–∫—É—â–∏–π –ø—Ä–æ—Ç–æ–∫–æ–ª (–µ—Å–ª–∏ –∂–∏—Ä–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–æ –∏–º—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞)
    current_profile_reports = Report.find_by_profile(g.current_profile.id)
    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    report_names = {report.report_name: report.id for report in current_profile_reports}

    for para in doc.paragraphs:
        if para.runs and para.runs[0].bold:  # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–æ–¥–µ—Ä–∂–∏—Ç –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç
            potential_report_name = para.text.strip()
            if potential_report_name in report_names:
                current_protocol = report_names[potential_report_name]  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º ID –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
            else:
                current_protocol = None  # –°–±—Ä–æ—Å, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

        else:
            # –†–∞–∑–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –∑–∞–ø—è—Ç–æ–π
            key_words = process_keywords(para.text)
            if key_words:
                keywords.append({
                    'report_id': current_protocol,  # –ï—Å–ª–∏ current_protocol = None, —ç—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    'key_words': key_words
                })

    return keywords


def process_keywords(key_word_input):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—è—Ç–æ–π, 
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫"""
    
    key_words = []
    for word in key_word_input.split(','):
        stripped_word = word.strip()
        if stripped_word:
            key_words.append(stripped_word)
    return key_words


def check_existing_keywords(key_words):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —É —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
    –≤ —Å–ª—É—á–∞–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∏–ª–∏ None, –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã.
    
    Args:
        key_words (list): –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.

    Returns:
        string or None: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –µ—Å–ª–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã, –∏–ª–∏ None, –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã.
    """
    
    profile_key_words = KeyWord.find_by_profile(g.current_profile.id)

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    existing_keywords = {}

    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    for kw in profile_key_words:
        key_word_lower = kw.key_word.lower()
        if key_word_lower in [kw.lower() for kw in key_words]:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–≤—è–∑–∞–Ω—ã –ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –æ—Ç—á–µ—Ç–∞–º–∏
            if kw.key_word_reports:
                for report in kw.key_word_reports:
                    report_name = report.report_name
                    if report_name not in existing_keywords:
                        existing_keywords[report_name] = []
                    existing_keywords[report_name].append(kw.key_word)
            else:
                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ —Å–≤—è–∑–∞–Ω–æ —Å –æ—Ç—á–µ—Ç–æ–º, –æ–Ω–æ —è–≤–ª—è–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–º
                if "global" not in existing_keywords:
                    existing_keywords["global"] = []
                existing_keywords["global"].append(kw.key_word)

    # –ï—Å–ª–∏ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã, —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    if existing_keywords:
        existing_keywords_message = []
        for key, words in existing_keywords.items():
            words_list = ", ".join(words)
            if key == "global":
                existing_keywords_message.append(f"global: {words_list}")
            else:
                existing_keywords_message.append(f"{key}: {words_list}")

        message = "The following keywords already exist:\n" + "\n".join(existing_keywords_message)
        return message

    return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ Word.
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ new_report_creation.py
def extract_paragraphs_and_sentences(file_path):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ Word.

    –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç .docx, –æ–ø—Ä–µ–¥–µ–ª—è—è –∞–±–∑–∞—Ü—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å –∂–∏—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤. 
    –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —ç—Ç–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–µ–≤ –º–æ–≥—É—Ç —Ä–∞–∑–¥–µ–ª—è—Ç—å—Å—è —Å–∏–º–≤–æ–ª–æ–º '!!', 
    —Ñ–æ—Ä–º–∏—Ä—É—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

    Args:
        file_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É Word.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏—Ö –∞–±–∑–∞—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞. 
              –ö–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç:
              - 'title' (str): –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∞–±–∑–∞—Ü–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å).
              - 'sentences' (list): –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∏–ª–∏ —Å–ø–∏—Å–∫–æ–≤ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).
              - 'visible' (bool): –í–∏–¥–∏–º–æ—Å—Ç—å –∞–±–∑–∞—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True).
              - 'bold' (bool): –Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∂–∏—Ä–Ω—ã–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False).
              - 'is_title' (bool): –ü–æ–º–µ—á–µ–Ω –ª–∏ –∞–±–∑–∞—Ü –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False).
              - 'type_paragraph' (str): –¢–∏–ø –∞–±–∑–∞—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "text").
    """
    document = Document(file_path)
    # –ü—Ä–æ–≤–µ—Ä—è—é –Ω–µ –ø—É—Å—Ç–æ–π –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
    if not document.paragraphs:
        return []
    
    paragraphs_from_file = []
    current_paragraph = None

    for para in document.paragraphs:
        # Check if paragraph has bold text indicating it's a new section
        if para.runs and para.runs[0].bold:
            # Handle previous paragraph before moving to the next one
            if current_paragraph:
                paragraphs_from_file.append(current_paragraph)
            
            # Create a new paragraph entry
            current_paragraph = {
                'title': para.text.strip(),
                'sentences': []
            }
        else:
            # Append sentences to the current paragraph
            if current_paragraph:
                sentences = para.text.strip().split('!!')
                # If there are multiple parts, store them as a nested list
                if len(sentences) > 1:
                    current_paragraph['sentences'].append([s.strip() for s in sentences])
                else:
                    current_paragraph['sentences'].append(sentences[0].strip())

    # Append the last paragraph
    if current_paragraph:
        paragraphs_from_file.append(current_paragraph)

    return paragraphs_from_file


# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞. –ü—Ä–æ–≤–µ—Ä—è—é –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. 
# –£–±–∏—Ä–∞—é —Ç–æ–ª—å–∫–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã. 
# –ò—Å–∫–ª—é—á–µ–Ω–∏–µ - –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ. 
# –î–æ–±–∞–≤–ª—è—é –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
def preprocess_sentence(text):
    """
    Performs a rough cleanup of the text to prepare it for further processing.
    Args:
        text (str): The input sentence or text.
    Returns:
        str: The preprocessed text.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –±—É–∫–≤—É –∏–ª–∏ —Ü–∏—Ñ—Ä—É
    if not re.search(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]', text):
        return ""

   # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏, –∫—Ä–æ–º–µ —Ç–æ—á–∫–∏
    text = re.sub(r'([,!?:;"\'\(\)])\1+', r'\1', text)  # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ ‚Üí –æ–¥–∏–Ω

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
    text = re.sub(r'\.\.\.\.+', '...', text)  # –ß–µ—Ç—ã—Ä–µ –∏ –±–æ–ª–µ–µ —Ç–æ—á–µ–∫ ‚Üí –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ
    text = re.sub(r'(?<!\.)\.\.(?!\.)', '.', text)  # –î–≤–µ —Ç–æ—á–∫–∏ ‚Üí –æ–¥–Ω–∞
    
    # –î–æ–±–∞–≤–ª—è—é –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    text = re.sub(r'([.!?,;:])(?!\s)', r'\1 ', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)  # –ó–∞–º–µ–Ω—è–µ–º –ª—é–±—ã–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª

    return text


# —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º 
# –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ working_with_reports.py –¥–≤–∞–∂–¥—ã
def clean_text_with_keywords(sentence, key_words, except_words=None):
    """ –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–ø–µ–Ω–∞–Ω–∏—è, 
    —Ü–∏—Ñ—Ä, —Å–ª–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º –≤—Å–µ—Ö —Å–ª–æ–≤ 
    –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É """
    # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∫ —Å—Ç—Ä–æ—á–Ω—ã–º –±—É–∫–≤–∞–º
    sentence = str(sentence)
    
    sentence = sentence.lower()
    # –£–¥–∞–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if key_words:
        for word in key_words:
            sentence = re.sub(rf'(?<!\w){re.escape(word)}(?!\w)', '', sentence, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –±—É–∫–≤ —Ü–∏—Ñ—Ä –∏ —Ä–æ–±–µ–ª–æ–≤
    sentence = re.sub(r"[^\w\s]", "", sentence)
    # –£–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—ã
    sentence = re.sub(r"\d+", "", sentence)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    sentence = re.sub(r"\s+", " ", sentence).strip()
    # –£–±–∏—Ä–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    if except_words:
        for word in except_words:
            sentence = re.sub(rf"(?<!\w){re.escape(word)}(?!\w)", "", sentence, flags=re.IGNORECASE)
    
    return sentence


# —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
# –≤ working_with_reports.py.
# –û–Ω–∞ –Ω–µ –æ—á–∏—â–∞–µ—Ç –¥–≤–æ–π–Ω—ã–µ –∑–Ω–∞–∫–∏, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ
# –î–æ–ª–∂–Ω–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –ø–æ—Å–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏ preprocess_sentence
def clean_and_normalize_text(text):
    """
    Cleans the input text by handling punctuation, spaces, and formatting issues.
    Args:
        text (str): The text to be cleaned.
    Returns:
        str: The cleaned and normalized text.
    """
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
    exeptions_after_punctuation =current_app.config["PROFILE_SETTINGS"]["EXCEPTIONS_AFTER_PUNCTUATION"]

    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–æ—á–∫–æ–π –∏–ª–∏ —Å–∫–æ–±–∫–æ–π –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
    # –≠—Ç–æ –≤ —Ç–µ–º—É –∏–º–µ–Ω–Ω–æ —Ç—É—Ç, —Ç–∞–∫ –∫–∞–∫ –Ω–∏–∂–µ —è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–∫–æ–±–∫–∏
    text = re.sub(r'^\s*\d+[\.\)]\s*', '', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+([.,!?;:])', r'\1', text)  # –ü—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'\(\s+', r'(', text)  # –ü—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏
    text = re.sub(r'\s+\)', r')', text)  # –ü—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–æ–π

    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏. 
    # –≠—Ç–æ –≤ —Ç–µ–º—É –∏–º–µ–Ω–Ω—É —Ç—É—Ç, —Ç–∞–∫ –∫–∞–∫ –≤—ã—à–µ –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã.
    text = text.strip()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
    if text and text[0].islower():
        text = text[0].upper() + text[1:]
        
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ –¥–≤–æ–µ—Ç–æ—á–∏–π –∏ –∑–∞–ø—è—Ç—ã—Ö
    def process_after_punctuation(match):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è.
        –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–µ –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è—Ö, –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –µ–≥–æ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä.
        """
        punctuation = match.group(1)
        word = match.group(2)
        if word in exeptions_after_punctuation:
            return f"{punctuation} {word}"  # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ –∫–∞–∫ –µ—Å—Ç—å
        return f"{punctuation} {word.lower()}"  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞
    text = re.sub(r'([,:]) (\w+)', process_after_punctuation, text)
        
    if not re.search(r'[.!?]$', text):
        text += '.'

    return text


# —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ working_with_reports.py
def split_sentences_if_needed(text):
    """
    Splits a sentence into multiple sentences using SpaCy tokenizer.
    Args:
        text (str): The input sentence text.
    Returns:
        tuple: (list of valid sentences, list of excluded sentences).
    """
    language = current_app.config.get("PROFILE_SETTINGS", {}).get("APP_LANGUAGE", "ru")
    print(f"APP_LANGUAGE: {language}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å SpaCy –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —è–∑—ã–∫–∞
    try:
        nlp = SpacyModel.get_instance(language)
    except ValueError as e:
        current_app.logger.error(f"Unsupported language '{language}' for SpaCy model: {e}")
        return [], []
    
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]
    for sentence in sentences:
        if not re.search(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]', sentence):
            sentences.remove(sentence)

    if len(sentences) > 1:
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        return [], sentences # Splitted sentences
    return sentences, [] # Unsplitted sentences



def sort_key_words_group(unsorted_key_words_group):
    """
    –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≥—Ä—É–ø–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ –ø–µ—Ä–≤–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –≥—Ä—É–ø–ø–µ.
    –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: —Å –æ—Ç—á–µ—Ç–∞–º–∏, —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ –∏ –±–µ–∑ –Ω–∏—Ö.
    """
    key_words_group_with_first_letter = []

    for group_data in unsorted_key_words_group:
        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤—ã–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        key_words = group_data.get("key_words", [])

        if key_words:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äî –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –æ–±—ä–µ–∫—Ç—ã —Å –ø–æ–ª–µ–º "word"
            if isinstance(key_words[0], dict):
                # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä–∏, –±–µ—Ä–µ–º –ø–æ–ª–µ "word"
                first_letter = key_words[0]["word"][0].lower() if key_words[0]["word"] else ""
            else:
                # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ‚Äî —ç—Ç–æ —Å—Ç—Ä–æ–∫–∏
                first_letter = key_words[0][0].lower() if key_words[0] else ""
        else:
            first_letter = ""  # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∏ —Å–∞–º—É –≥—Ä—É–ø–ø—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        key_words_group_with_first_letter.append((first_letter, group_data))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ –ø–µ—Ä–≤–æ–π –±—É–∫–≤–µ
    sorted_key_words_group = sorted(key_words_group_with_first_letter, key=lambda x: x[0])

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã (–±–µ–∑ –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã)
    return [group_data for _, group_data in sorted_key_words_group]


def group_keywords(keywords, with_index=False, with_report=False):
    """
    –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π 
    —Å –∫–ª—é—á–∞–º–∏ group_index –∏ key_words –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö 
    –Ω–∞ –≤—Ö–æ–¥–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –æ—Ç—á–µ—Ç–∞–º–∏,
    –µ—Å–ª–∏ with_report=True.

    Args:
        keywords (list): –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ KeyWordGroup.
        with_index (boolean): –§–ª–∞–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –∏–ª–∏ –±–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞.
        with_report (boolean): –§–ª–∞–≥ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –æ—Ç—á–µ—Ç–∞–º.
    
    Returns:
    with_index=True:
        list of dict: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ group_index –∏ key_words (–≤–∫–ª—é—á–∞–µ—Ç id –∏ —Å–ª–æ–≤–æ).
    with_index=False:
        list of lists: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ group_index –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ (–≤–∫–ª—é—á–∞–µ—Ç id –∏ —Å–ª–æ–≤–æ).
    with_report=True:
        list of dict: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –≥–¥–µ –∫–ª—é—á–∏ report_id, report_name, group_index –∏ key_words (–≤–∫–ª—é—á–∞–µ—Ç id –∏ —Å–ª–æ–≤–æ).
    """
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ group_index, –¥–æ–±–∞–≤–ª—è—è –∏—Ö id –∏ —Å–ª–æ–≤–æ
    grouped_keywords = defaultdict(list)
    for keyword in keywords:
        grouped_keywords[keyword.group_index].append({'id': keyword.id, 'word': keyword.key_word})

    if with_report:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –æ—Ç—á–µ—Ç–∞–º
        report_key_words = []
        seen_groups = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø

        for keyword in keywords:
            group_index = keyword.group_index
            group_words = grouped_keywords[group_index]

            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç—á–µ—Ç–∞, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
            for report in keyword.key_word_reports:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª–∞ –ª–∏ –≥—Ä—É–ø–ø–∞ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –¥–ª—è —ç—Ç–æ–≥–æ –æ—Ç—á–µ—Ç–∞
                if (report.id, group_index) not in seen_groups:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å—é –≥—Ä—É–ø–ø—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –µ—Å–ª–∏ –µ—ë –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–∏–ª–∏
                    report_key_words.append({
                        'report_id': report.id,
                        'report_name': report.report_name,
                        'group_index': group_index,
                        'key_words': group_words  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å—é –≥—Ä—É–ø–ø—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –∏—Ö id
                    })
                    # –û—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –≥—Ä—É–ø–ø–∞ —Å —ç—Ç–∏–º group_index —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
                    seen_groups.add((report.id, group_index))

        return report_key_words

    # –ï—Å–ª–∏ with_index=True, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å group_index
    if with_index:
        grouped_keywords_with_index = []
        for group_index, words in grouped_keywords.items():
            grouped_keywords_with_index.append({'group_index': group_index, 'key_words': words})
        return grouped_keywords_with_index
    
    # –ï—Å–ª–∏ with_index=False, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –∏—Ö id
    return list(grouped_keywords.values())


# –°—Ä–∞–≤–Ω–∏–≤–∞—é 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ working_with_report/save_modified_sentences %
def compare_sentences_by_paragraph(new_sentences, report_id):    
    """
    Compares new sentences with existing sentences in their respective paragraphs to determine uniqueness.

    Args:
        existing_paragraphs (list[Paragraph]): List of Paragraph objects with related sentences.
        new_sentences (list[dict]): List of new sentences to be added.
            Each dictionary should have the structure {"paragraph_id": int, "text": str}.
        key_words (list[str]): List of key words to remove during cleaning.

    Returns:
        dict: Contains "duplicates" and "unique" lists:
            - "duplicates": List of new sentences that match existing ones.
            - "unique": List of new sentences considered unique.
    """
    similarity_threshold_fuzz = float(current_app.config["PROFILE_SETTINGS"]["SIMILARITY_THRESHOLD_FUZZ"])
    except_words = current_app.config["PROFILE_SETTINGS"]["EXCEPT_WORDS"]
    print(f"SIMILARITY_THRESHOLD_FUZZ: {similarity_threshold_fuzz}")
    print(f"EXCEPT_WORDS: {except_words}")
    
    existing_paragraphs = Paragraph.query.filter_by(report_id=report_id).all()
    key_words_obj = KeyWord.get_keywords_for_report(g.current_profile.id, report_id)
    key_words = [keyword.key_word for keyword in key_words_obj]
    
    duplicates = []
    unique_sentences = []
    errors_count = 0
    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    for new_sentence in new_sentences:
        new_paragraph_id = int(new_sentence.get("paragraph_id"))
        new_text = new_sentence.get("text")
        new_text_index = new_sentence.get("sentence_index")
        
        if not new_paragraph_id or not new_text:
            errors_count += 1
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        paragraph = next((p for p in existing_paragraphs if p.id == new_paragraph_id), None)
        
        if not paragraph:
            unique_sentences.append(new_sentence)  # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å—á–∏—Ç–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º
            continue

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ —á–µ–π –∏–Ω–¥–µ–∫—Å —Ä–∞–≤–µ–Ω –∏–Ω–¥–µ–∫—Å—É –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        existing_sentences = [
            {"id": sent.id, "index": sent.index, "text": sent.sentence} for sent in paragraph.paragraph_to_sentences if sent.index == new_text_index
        ]
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        cleaned_existing = [
            {"id": sent.get("id"), "original_text": sent["text"], "cleaned_text": clean_text_with_keywords(sent.get("text"), key_words, except_words)} for sent in existing_sentences
            ]
        # –û—á–∏—â–∞–µ–º –Ω–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        cleaned_new_text = clean_text_with_keywords(new_text, key_words, except_words)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏
        is_duplicate = False
        for existing in cleaned_existing:
            similarity_rapidfuzz = fuzz.ratio(cleaned_new_text, existing["cleaned_text"])
            if similarity_rapidfuzz >= similarity_threshold_fuzz:
                duplicates.append({
                    "new_sentence": new_sentence,
                    "new_sentence_index": new_sentence.get("sentence_index"),
                    "new_sentence_paragraph": new_sentence.get("paragraph_id"),
                    "matched_with": {
                                "id": existing["id"],
                                "text": existing["original_text"]
                            },
                    "similarity_rapidfuzz": similarity_rapidfuzz
                })
                is_duplicate = True
                break

        if not is_duplicate:
            unique_sentences.append(new_sentence)

    return {"duplicates": duplicates, "unique": unique_sentences, "errors_count": errors_count}


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é –≤ models.py
def find_similar_exist_sentence(sentence_text, sentence_type, report_type_id, user_id, tags=None, comment=None):
    """
    Finds similar sentences of the same type in the database.

    Args:
        sentence_text (str): The text of the sentence to compare.
        sentence_type (str): The type of the sentence.

    Returns:
        similar sentence.
    """
    logger.info(f"(—Ñ—É–Ω–∫—Ü–∏—è find_similar_exist_sentence)(—Ç–∏–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: '{sentence_type}') üöÄ –ù–∞—á–∞—Ç –ø–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞ –∏ —Å —Ç–∞–∫–∏–º–∏ –∂–µ –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    if sentence_type == "head":
        similar_type_sentences = HeadSentence.query.filter_by(tags=tags, comment=comment, report_type_id=report_type_id, user_id = user_id).all()
    elif sentence_type == "body":
        similar_type_sentences = BodySentence.query.filter_by(tags=tags, comment=comment, report_type_id=report_type_id, user_id = user_id).all()
    elif sentence_type == "tail":
        similar_type_sentences = TailSentence.query.filter_by(tags=tags, comment=comment, report_type_id=report_type_id, user_id = user_id).all()
    else:
        raise ValueError(f"Invalid sentence type: {sentence_type}")
    
    logger.info(f"(—Ñ—É–Ω–∫—Ü–∏—è find_similar_exist_sentence) –ù–∞–π–¥–µ–Ω–æ {len(similar_type_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º")    
    
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –∫–∞–∂–¥—ã–º –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
    for exist_sentence in similar_type_sentences:
        if exist_sentence.sentence == sentence_text:
            logger.info(f"(—Ñ—É–Ω–∫—Ü–∏—è find_similar_exist_sentence) üß© –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º '{exist_sentence.sentence}' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –í–æ–∑–≤—Ä–∞—â–∞—é –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
            return exist_sentence
    logger.info(f"(—Ñ—É–Ω–∫—Ü–∏—è find_similar_exist_sentence) –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –í–æ–∑–≤—Ä–∞—â–∞—é None")
    return None
    
      
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –≥–ª–∞–≤–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞. –ù–æ–º–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–µ –¥–æ–ª–∂–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è
def check_head_sentence_indexes(paragraph_id):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –≥–ª–∞–≤–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ.
    """
    head_sentences_groupe, _ = Paragraph.get_paragraph_groups(paragraph_id)
    
    head_sentences = HeadSentenceGroup.get_sentences(head_sentences_groupe)
    indexes = [sent.index for sent in head_sentences]
    duplicates = [index for index in indexes if indexes.count(index) > 1]
    return duplicates

        
      
        