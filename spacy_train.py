# spacy_train.py

import spacy
from spacy.tokens import DocBin
from spacy.training import Example
import json
import os
from logger import logger
import glob
import random
import datetime

MODEL_NAME = "ru_core_news_sm"
OUTPUT_DIR = "models/custom_model"


def start_spacy_retrain() -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—É—é –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å SpaCy, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞—Ä—Ö–∏–≤ –∏ –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã,
    –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    """

    logger.info("üß† –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ SpaCy")

    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    data_dir = "spacy_training_data"
    current_path = os.path.join(data_dir, "sent_boundary.jsonl")
    archive_path = os.path.join(data_dir, "sent_boundary_all.jsonl")
    training_tmp = os.path.join(data_dir, "combined_training.jsonl")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    combined = []
    if os.path.exists(archive_path):
        with open(archive_path, "r", encoding="utf-8") as f:
            combined += f.readlines()
    if os.path.exists(current_path):
        with open(current_path, "r", encoding="utf-8") as f:
            combined += f.readlines()

    if len(combined) < 3:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    random.shuffle(combined)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    with open(training_tmp, "w", encoding="utf-8") as f:
        f.writelines(combined)

    logger.info(f"üì¶ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(combined)} —Å—Ç—Ä–æ–∫–∞—Ö")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—É—é
    last_model_path = None
    try:
        last_model_path = sorted(
            glob.glob("spacy_models/custom_sentencizer_v*"),
            key=os.path.getmtime,
            reverse=True
        )[0]
    except IndexError:
        logger.warning("–ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è.")

    if last_model_path:
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ {last_model_path}")
        nlp = spacy.load(last_model_path)
    else:
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å {MODEL_NAME}")
        nlp = spacy.load(MODEL_NAME)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    doc_bin = DocBin()
    for line in combined:
        entry = json.loads(line)
        doc = nlp.make_doc(entry["text"])
        sent_starts = entry["sent_starts"]

        if len(sent_starts) != len(doc):
            logger.warning(f"‚ö†Ô∏è –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤: —Ç–µ–∫—Å—Ç: '{entry['text'][:60]}...' "
                        f"(—Ç–æ–∫–µ–Ω–æ–≤: {len(doc)}, —Ä–∞–∑–º–µ—Ç–∫–∏: {len(sent_starts)})")
            continue

        for i, token in enumerate(doc):
            token.is_sent_start = sent_starts[i]

        doc_bin.add(doc)

    examples = []
    for doc in doc_bin.get_docs(nlp.vocab):
        example = Example(doc, doc)
        examples.append(example)

    # –û–±—É—á–∞–µ–º
    nlp.initialize(get_examples=lambda: examples)
    nlp.update(examples)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_output_path = f"spacy_models/custom_sentencizer_v{int(datetime.datetime.now().timestamp())}"
    os.makedirs(model_output_path, exist_ok=True)
    nlp.to_disk(model_output_path)

    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_output_path}")

    return model_output_path